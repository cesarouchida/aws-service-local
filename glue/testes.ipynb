{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3e8f592-9a0b-40e6-94d5-c301f2e829c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      "|-- Product: string\n",
      "|-- Qty: string\n",
      "\n",
      "+-------+---+\n",
      "|Product|Qty|\n",
      "+-------+---+\n",
      "|     P1| 10|\n",
      "|     P2|  5|\n",
      "+-------+---+\n",
      "\n",
      "+---+------+-----+\n",
      "| id|  nome|idade|\n",
      "+---+------+-----+\n",
      "|  1|   Ana|   60|\n",
      "|  2|Carine|   26|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "gc = GlueContext(sc)\n",
    "spark = gc.spark_session\n",
    "\n",
    "plist = '[{\"Product\":\"P1\",\"Qty\":\"10\"},{\"Product\":\"P2\",\"Qty\":\"5\"}]'\n",
    "\n",
    "spark_csv = spark.read.option(\"delimiter\", \";\").csv(\"teste.csv\", header=True)\n",
    "spark_df = spark.read.json(sc.parallelize([plist]))\n",
    "glue_df_csv = DynamicFrame.fromDF(spark_csv, gc, \"glue_df\")\n",
    "glue_df = DynamicFrame.fromDF(spark_df, gc, \"glue_df\")\n",
    "\n",
    "glue_df.printSchema()\n",
    "glue_df.toDF().show()\n",
    "\n",
    "glue_df_csv.toDF().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a823496-b794-4a56-8bf2-ac600196a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08609b8b-ab25-4b83-bbef-27e7e90e710d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_bucket(bucket_name: str, file_name: str):\n",
    "    try:\n",
    "        # host.docker.internal\n",
    "        s3 = boto3.client('s3',\n",
    "                          endpoint_url=\"http://host.docker.internal:4566\",\n",
    "                          use_ssl=False,\n",
    "                          aws_access_key_id='mock',\n",
    "                          aws_secret_access_key='mock',\n",
    "                          region_name='us-east-1')\n",
    "        s3.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "        file_key = f'{os.getcwd()}/{file_name}'\n",
    "        with open(file_key, 'rb') as f:\n",
    "            s3.put_object(Body=f, Bucket=bucket_name, Key=file_name)\n",
    "        print(file_name)\n",
    "\n",
    "        return s3\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2369639-5aa4-4e41-aa29-b5c2317c115d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_testing_pyspark_session():\n",
    "    print('creating pyspark session')\n",
    "    sparksession = (SparkSession.builder\n",
    "                    .master('local[2]')\n",
    "                    .appName('pyspark-demo')\n",
    "                    .enableHiveSupport()\n",
    "                    .getOrCreate())\n",
    "\n",
    "    hadoop_conf = sparksession.sparkContext._jsc.hadoopConfiguration()\n",
    "    hadoop_conf.set(\"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "    hadoop_conf.set(\"fs.s3a.path.style.access\", \"true\")\n",
    "    hadoop_conf.set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "    hadoop_conf.set(\"com.amazonaws.services.s3a.enableV4\", \"true\")\n",
    "    hadoop_conf.set(\"fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.TemporaryAWSCredentialsProvider\")\n",
    "    hadoop_conf.set(\"fs.s3a.access.key\", \"mock\")\n",
    "    hadoop_conf.set(\"fs.s3a.secret.key\", \"mock\")\n",
    "    hadoop_conf.set(\"fs.s3a.session.token\", \"mock\")\n",
    "    hadoop_conf.set(\"fs.s3a.endpoint\", \"http://host.docker.internal:4566\")\n",
    "    return sparksession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df6c22c-efea-4a5c-96ed-8545189118e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "teste.csv\n",
      "creating pyspark session\n",
      "+---+------+-----+\n",
      "| id|  nome|idade|\n",
      "+---+------+-----+\n",
      "|  1|   Ana|   60|\n",
      "|  2|Carine|   26|\n",
      "+---+------+-----+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "test_bucket = 'teste'\n",
    "# Write to S3 bucket\n",
    "add_to_bucket(bucket_name=test_bucket, file_name='teste.csv')\n",
    "spark_session = create_testing_pyspark_session()\n",
    "file_path = f's3://{test_bucket}/teste.csv'\n",
    "\n",
    "# Read from s3 bucket\n",
    "data_df = spark_session.read.option('delimiter', ';').option('header', 'true').option('inferSchema',\n",
    "                                                                                      'False').format('csv').load(file_path)\n",
    "print(data_df.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5a10853-36e5-4dbc-92ec-02c39ba7d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_path = f's3a://{test_bucket}/testparquet/'\n",
    "data_df.write.parquet(write_path, mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e1d5f9-c836-41dd-9c18-81ef5a655b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}